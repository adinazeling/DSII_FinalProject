---
title: "Predicting Gender from Risk-Seeking Behaviors"
author: "Deepika Dilip, Rachel Tsong, and Adina Zhang"
date: "May 14, 2019"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(caret)
library(ranger)
library(xgboost)
library(pROC)

# Load Datset
dat1 = read_csv("./final.csv") %>%
  dplyr::select(-X1) %>%
  mutate(gender = ifelse(gender == 0, "female", "male"))

set.seed(1)
# Partition dataset into training and testing datasets
row_train = createDataPartition(y = dat1$gender, p = 0.8, list = FALSE)
dat1_train = dat1[row_train,]
dat1_test = dat1[-row_train,]

# Set up cross-validation for tuning parameters
ctrl = trainControl(method = "repeatedcv",
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)
```

## Introduction

One public health phenomenon that has been the subject of debate is the “Gender and Health Paradox”, in which women experience decreased quality of life, yet have higher life expectancies than men. A theory that seeks to explain this pattern outlines increased risk-taking by men as a causal mechanism. By this logic, men would indicate more risk-taking behaviors than women when controlled for age and other confounders. We sought to test this theory by examining if risk-seeking behaviors could be predictive of gender. 

**Young People's Survey**

For our analysis, we used a data set consisting of 1,010 responses from a 2013 survey administered among statistics students and their friends at Comenius University in Bratislava, Slovakia. This dataset is available to download from Kaggle. The survey consisted of 150 items including music and movie preferences, hobbies and interests, phobias, health habits, personality traits, views on life, and opinions, spending habits, and demographics. Most of the items were presented as a five-point Likert scale indicating how much a respondent agreed to the statement. Participants were given the survey in both electronic and paper formats. To test our hypotheses, we selected 19 variables that would demonstrate risk-seeking behavior.

After sectioning the data accordingly, we counted missing values to determine if that could affect our results. When only completed responses were considered (i.e. having values for every variable), we had a sample size of 942; 6.7% of the data was missing. As this value was less than 10%, we decided not to address this via imputation and opted for a deletion method (complete-case analysis). The limitations of this method will be addressed in the discussion.

## Data Exploration

## Method

We selected 19 variables that were related to thrill or risk seeking behaviors. The following variables were included in our analysis:
*  Interest in: (ranked 1-5 from "don't enjoy at all" to "enjoy very much")
  +  Metal, hard rock music
  +  Punk music
  +  Horror movies
  +  War movies
  +  Action movies
  +  Outdoor activities
  +  Sport at a competitive level
  +  Adrenaline sports
  +  Cars
*  Fears of public speaking (ranked 1-5 from "not afraid at all" to "very afraid of")
*  Smoking habits (categorical with 4 levels: "never smoked", "tried smoking", "former smoker", "current smoker")
*  Drinking habits (categorical with 3 levels: "never", "social drinker", "drink a lot")
*  Statements about personality traits: (ranked 1-5 from "strongly disagree" to "strongly agree")
  +  I damaged things in the past when I get angry
  +  I used to cheat at school
  +  I prefer big dangerous dogs to smaller, calmer dogs
  +  I try to do tasks as soon as possible and not leave them until last minute
  +  I'm not afraid to give my opinion if I feel strongly about something
  +  I always listen to my parents' advice
  
Variables that were ranked from on a scale from 1-5 were considered as continous responses. After splitting the data into training and testing sets using an 80/20 split, we fit the following models to the training data:
*  Logistic regression
*  Discriminant analysis (linear and quadratic)
*  KNN
*  Support vector machine (linear and radial kernels)
*  Classification trees (random forest and boosting)

Model performance was assessed on the training data using the `resamples()` function in the `caret` package.

## Models

### Logistic Regression

```{r, include = FALSE}
set.seed(1) 
# Fit a logistic regression 
log.fit = train(gender~., dat1_train,
                 method = "glm",
                 metric = "ROC",
                 trControl = ctrl)
```

### Discriminant Analysis

**Linear**

```{r, include = FALSE}
set.seed(1) 
# Fit LDA model
lda.fit = train(gender~., dat1_train, 
                   method = "lda", 
                   metric = "ROC", 
                   trControl = ctrl)
```

**Quadratic**

```{r, include = FALSE}
set.seed(1) 
# Fit QDA model
qda.fit <- train(gender~., dat1_train, 
                   method = "qda", 
                   metric = "ROC", 
                   trControl = ctrl)
```

### KNN

```{r, message = FALSE, warning = FALSE, include = FALSE, cache = TRUE}
knn.fit = train(x = dat1_train[,1:19],
                y = dat1_train$gender,
                method = "knn",
                preProcess = c("center", "scale"),
                tuneGrid = data.frame(k = seq(50, 120, by = 2)),
                trControl = ctrl)
```

### SVM 

**Linear**

```{r, include = FALSE, cache = TRUE}
set.seed(1)
# Run linear SVM
svm_linear = train(gender ~ . ,
                   data = dat1_train,
                   method = "svmLinear2",
                   preProcess = c("center", "scale"),
                   tuneGrid = data.frame(cost = exp(seq(-5, 1, len = 30))),
                   metric = "ROC",
                   trControl = ctrl)
```

**Radial**

```{r, include = FALSE, cache = TRUE}
# Set tuning grid
radial_grid = expand.grid(C = exp(seq(-4, 5, len = 15)),
                          sigma = exp(seq(-8, -5, len = 5)))

set.seed(1)
# Fit radial SVM model
svm_radial = train(gender ~ .,
                   data = dat1_train,
                   method = "svmRadial",
                   preProcess = c("center", "scale"),
                   tuneGrid = radial_grid,
                   metric = "ROC",
                   trControl = ctrl)
```


### Random Forest

```{r, include = FALSE, cache = TRUE}
# Set up tuning grid for random forest
rf.grid = expand.grid(mtry = 1:5,
                      splitrule = "gini",
                      min.node.size = 1:10)

set.seed(1)
# Run Random Forest
rf.fit = train(gender~., dat1_train,
               method = "ranger",
               tuneGrid = rf.grid, 
               trControl = ctrl,
               metric = "ROC",
               importance = "impurity")
```

### Extreme Gradient Boosting

```{r, include = FALSE, cache = TRUE}
# Set tuning grid
xgbGrid = expand.grid(nrounds = seq(from = 50, to = 200, by = 50),  
                      max_depth = c(2, 3, 4, 5, 6),
                      colsample_bytree = seq(0.5, 0.9, length.out = 5),
                      eta = 0.1,
                      gamma = 0,
                      min_child_weight = 1,
                      subsample = 1)

set.seed(1)
# Run boosting model using xgboost method
xgb.fit = train(gender~., dat1_train,
                trControl = ctrl,
                tuneGrid = xgbGrid, 
                method = "xgbTree",
                metric = "ROC",
                importance = "impurity")
```

## Model Selection 

```{r }
resamp = resamples(list(LOG = log.fit, LDA = lda.fit,
                        QDA = qda.fit, KNN = knn.fit,
                        SVML = svm_linear, SVMR = svm_radial,
                        RF = rf.fit, XGB = xgb.fit))
bwplot(resamp)
```


## Final Model

## Conclusion
Some of the limitations of this report include translation issues. The original language of the survey was Slovakian, and this could influence interpretations. Additionally, this affects generalizability of findings as the population of this survey was specifically statistics students in Slovakia. The deletion method used would also present issues if data were not missing at random, resulting in bias.

\newpage

## Appendix



