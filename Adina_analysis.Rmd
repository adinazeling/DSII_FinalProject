---
title: "Classification Trees with Random Forest and Boosting"
author: "Adina Zhang"
date: "May 9, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ranger)
library(xgboost)
library(gbm)
```

```{r}
# Load dataset
dat1 = read_csv("final.csv") %>% 
  select(-X1) %>% 
  mutate(gender = factor(gender))

set.seed(1)
# Partition dataset into training and testing datasets
row_train = createDataPartition(y = dat1$gender, p = 0.8, list = FALSE)
dat1_train = dat1[row_train,]
dat1_test = dat1[-row_train,]
```

## Random Forest

```{r}
# Set up cross-validation for caret package
ctrl = trainControl(method = "cv", number = 10)

# Set up tuning grid for random forest
rf.grid = expand.grid(mtry = 1:5,
                      splitrule = "gini",
                      min.node.size = 1:10)

set.seed(1)
# Run Random Forest
rf.fit = train(gender~., dat1_train,
               method = "ranger",
               tuneGrid = rf.grid, 
               trControl = ctrl,
               importance = "impurity")

# Plot of accuracy with different tuning parameters
ggplot(rf.fit, highlight = TRUE) + 
  labs(
    title = "Accuracy with tuning parameters"
  ) + 
  theme_bw()
```

This plot shows that the model with four predictors and a minimal node size of 1 yields the best model.

```{r}
# Variable importance
varImp(rf.fit)
```


```{r}
# Prediction
rf.pred = predict(rf.fit, newdata = dat1_test)
confusionMatrix(data = rf.pred,
                reference = dat1_test$gender,
                positive = "1")
```

## Boosting

```{r}
# Set tuning grid
xgbGrid <- expand.grid(nrounds = c(100,200),  
                       max_depth = c(10, 15, 20, 25),
                       colsample_bytree = seq(0.5, 0.9, length.out = 5))
```

