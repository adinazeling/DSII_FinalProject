---
title: "Classification Trees with Random Forest and Boosting"
author: "Adina Zhang"
date: "May 9, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(ranger)
library(xgboost)
library(gbm)
```

```{r}
# Load dataset
dat1 = read_csv("final.csv") %>% 
  select(-X1) %>% 
  mutate(gender = factor(gender))

set.seed(1)
# Partition dataset into training and testing datasets
row_train = createDataPartition(y = dat1$gender, p = 0.8, list = FALSE)
dat1_train = dat1[row_train,]
dat1_test = dat1[-row_train,]
```

## Random Forest

```{r}
# Set up cross-validation for caret package
ctrl = ctrl = trainControl(method = "repeatedcv",
                           summaryFunction = twoClassSummary,
                           classProbs = TRUE)

# Set up tuning grid for random forest
rf.grid = expand.grid(mtry = 1:5,
                      splitrule = "gini",
                      min.node.size = 1:10)

set.seed(1)
# Run Random Forest
rf.fit = train(gender~., dat1_train,
               method = "ranger",
               tuneGrid = rf.grid, 
               trControl = ctrl,
               metric = "ROC",
               importance = "impurity")

# Plot of accuracy with different tuning parameters
ggplot(rf.fit, highlight = TRUE) + 
  labs(
    title = "Accuracy with tuning parameters"
  ) + 
  theme_bw()
```

This plot shows that the model with four predictors and a minimal node size of 1 yields the best model.

```{r}
# Variable importance
varImp(rf.fit)
```


```{r}
# Prediction
rf.pred = predict(rf.fit, newdata = dat1_test)
confusionMatrix(data = rf.pred,
                reference = factor(dat1_test$gender),
                positive = "male")
```

## Boosting

```{r}
# Set tuning grid
xgbGrid = expand.grid(nrounds = seq(from = 50, to = 200, by = 50),  
                      max_depth = c(2, 3, 4, 5, 6),
                      colsample_bytree = seq(0.5, 0.9, length.out = 5),
                      eta = 0.1,
                      gamma = 0,
                      min_child_weight = 1,
                      subsample = 1)

set.seed(1)
# Run boosting model using xgboost method
xgb.fit = train(gender~., dat1_train,
                trControl = ctrl,
                tuneGrid = xgbGrid, 
                method = "xgbTree",
                metric = "ROC",
                importance = "impurity")

# Plot of accuracy with different tuning parameters
ggplot(xgb.fit, highlight = TRUE) + 
  labs(
    title = "Accuracy with tuning parameters"
  ) + 
  theme_bw()

```

Initial best tune parameters:
nrounds - 100
max_depth - 2
eta - 0.1
gamma - 0
colsample_bytree - 0.8
min_child_weight - 1
subsample - 1

```{r}
# Variable importance
xgb_imp = xgb.importance(feature_names = xgb.fit$finalModel$feature_names,
                         model = xgb.fit$finalModel)
xgb.ggplot.importance(xgb_imp) + ggtitle("Variable Importance") + theme_bw()
varImp(xgb.fit)
```

```{r}
# Prediction
boost.pred = predict(xgb.fit, newdata = dat1_test)
confusionMatrix(data = boost.pred,
                reference = factor(dat1_test$gender),
                positive = "male")
```

## Model Comparison

```{r}
resamp = resamples(list(rf = rf.fit, xgb = xgb.fit))
summary(resamp)
bwplot(resamp)
```

